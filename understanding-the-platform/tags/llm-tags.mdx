---
title: "LLM Tags"
description: "Tags generated through prompt-based inference"
---

## LLM Tags

LLM tags are generated through prompt-based inference. The agent analyzes conversation context and applies tags based on its understanding.

## How LLM Tags Work

The agent:
1. **Analyzes context** — Reviews conversation history and current turn
2. **Applies reasoning** — Uses LLM to understand the situation
3. **Generates tags** — Applies tags based on its analysis
4. **Maintains consistency** — Uses the tag prompt to guide decisions

## Tag Prompt

The tag prompt defines how the LLM should apply tags. It:
- **Defines tag categories** — What tags are available
- **Provides examples** — Shows when to apply tags
- **Sets criteria** — Specifies conditions for tag application
- **Guides reasoning** — Helps the LLM make consistent decisions

## Benefits

LLM tags provide:
- **Nuanced understanding** — Can interpret complex situations
- **Context awareness** — Considers full conversation context
- **Flexibility** — Can handle edge cases and variations
- **Natural language** — Understands intent and meaning

## Limitations

LLM tags have some limitations:
- **Cost** — Requires LLM calls for each tag decision
- **Latency** — Takes time to generate
- **Variability** — May not be 100% consistent
- **Complexity** — Harder to debug why tags were applied

## Best Practices

- **Clear prompts** — Write clear, specific tag prompts
- **Provide examples** — Include examples in the prompt
- **Test thoroughly** — Validate tag application
- **Monitor consistency** — Track tag application patterns
- **Combine with deterministic** — Use deterministic tags when possible

LLM tags are powerful for nuanced, context-aware tagging that requires understanding intent and meaning.

